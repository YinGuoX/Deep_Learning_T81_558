{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part3.1_Neural_net.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMt1VeW+oOzDMQesMVeU5J5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YinGuoX/Deep_Learning_T81_558/blob/master/Part3_1_Neural_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmjnBlWgB492"
      },
      "source": [
        "# Part3.1 : Deep Learning and Neural Network Introduction\n",
        "神经网络是最早的机器学习模型之一，它的流行程度已经下降过两次，现在是第三次上升期。深度学习也就意味着使用神经网络。深度学习中的‘深度’是指具有很多隐含层的神经网络。\n",
        "\n",
        "由于神经网络已经存在很长时间，因此它有许多不同方向的发展。研究人员创建了许多不同的训练算法、激活函数、传递函数、神经网络的结构。在本课程中只涉及深度神经网络的最行技术。将不会花时间去讨论神经网络的历史。如果你想了解神经网络的一些更经典的结构，请自行拓展。对于最新的技术，我为The Society of Actuaries on deep learning 写了一篇关于深度学习作为第三代神经网络的文章:[ third generation of neural networks.](https://www.soa.org/globalassets/assets/Library/Newsletters/Predictive-Analytics-and-Futurism/2015/december/paf-iss12.pdf)\n",
        "\n",
        "\n",
        "神经网络接收输入并且产生输出。神经网络的输入被称为**特征向量**，此向量的大小是固定长度。如果改变特征向量的大小，就意味着需要重新创建一个神经网络。\n",
        "\n",
        "尽管特征向量被称为“向量”，但是并非如此。 向量表示一维数组。 从历史上看，神经网络的输入始终为一维。 但是，在现代神经网络中，您可能会看到多维的输入数据，例如：\n",
        "\n",
        "* 1D Vector:神经网络的经典输入，类似于电子表格中的行。常见的预测建模。\n",
        "* 2D Matrix:灰度图像输入到卷积神经网络(CNN)。\n",
        "* 3D Matrix:彩色图像输入到卷积神经网络(CNN)。\n",
        "* nD Matrix:更高阶的数据输入到卷积神经网络(CNN)。\n",
        "\n",
        "在CNN产生之前，程序仅通过将图像的行并排放置，将图像矩阵压缩成一个长数组，就将图像输入发送到神经网络。 CNN不同，CNN是直接将nD矩阵穿过神经网络层。\n",
        "\n",
        "最初，本课程将重点关注神经网络的一维输入。 但是，以后的课程将更多地关注于更高维度的输入。\n",
        "\n",
        "**Dimensions**:维度这个术语在神经网络中可能会令人困惑。在一维输入向量的意义上，维数指的是一维数组中有多少元素。例如，一个有十个输入神经元的神经网络有十个维度。然而，现在我们有了CNN，输入也有了维度。神经网络的输入通常有1维、2维或3维。四个或更多维度是较不常见的。你可能有一个64x64像素的神经网络的2D输入。这种结构会产生4096个输入神经元。这个网络可以是2D的，也可以是4096d的，这取决于你引用的是哪一组维度。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfvP2DXwF1zC"
      },
      "source": [
        "## 1. 分类or回归\n",
        "像许多模型一样，神经网络可以用于分类或回归:\n",
        "* Regression：期望数字作为神经网络的预测。\n",
        "* Classification：您期望类/类别作为神经网络的预测。\n",
        "\n",
        "分类和回归神经网络如图3.CLS-REG所示。\n",
        "\n",
        "Figure 3.CLS-REG: Neural Network Classification and Regression \n",
        "![Neural Network Classification and Regression](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_2_ann_class_reg.png \"Neural Network Classification and Regression\")\n",
        "\n",
        "**注：**回归神经网络的输出是数字，分类的输出是一个类。 回归或两类分类，网络始终只有一个输出。 分类神经网络为每个类别都有一个输出神经元。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPAguyEgGazh"
      },
      "source": [
        "## 2. 神经元和层\n",
        "大多数神经网络结构使用某种类型的神经元。神经网络的种类很多，编程人员一直在引入实验神经网络结构。因此，不可能覆盖每一个神经网络结构。然而，神经网络的实现有一些共性。一个被称为神经网络的算法通常是由独立的、相互连接的单元组成的，即使这些单元可能被称为神经元，也可能不被称为神经元。神经网络处理单元的名称因文献来源的不同而有所不同。它可以被称为节点、神经元或单位。\n",
        "\n",
        "图3所示为单个人工神经元的抽象结构\n",
        "**Figure 3.ANN: An Artificial Neuron**\n",
        "![An Artificial Neuron](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_2_abstract_nn.png \"An Artificial Neuron\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOmz4nqnHVKi"
      },
      "source": [
        "人工神经元接收来自一个或多个来源的输入，这些来源可能是其他神经元，也可能是从计算机程序输入到网络中的数据。这种输入通常是浮点或二进制。通常，二进制输入被编码为浮点数，用1或0表示真或假。有时程序也描述二进制输入使用双极系统，true为1,false为-1。\n",
        "\n",
        "人工神经元将每个输入乘以权重。 然后，将这些乘法相加并将此和传递给激活函数。 一些神经网络不使用激活函数。 以下等式总结了神经元的计算输出：\n",
        "\n",
        "$f(x,w) = \\phi(\\sum_i(\\theta_i \\cdot x_i))$\n",
        "\n",
        "在上面的方程中，变量$x$和$\\theta$代表了神经元的输入和权重。变量$i$对应权重和输入的数量。您必须始终拥有与输入相同的权重数。神经网络将每个权值乘以其各自的输入，并将这些乘法的乘积输入到一个激活函数中，该激活函数用希腊字母表示为$\\phi$()。这个过程的结果是神经元产生一个单一的输出。\n",
        "\n",
        "上述神经元有两个输入加上第三个偏置。该神经元可以接受以下输入特征向量：\n",
        "\n",
        "$[1,2]$\n",
        "\n",
        "因为存在一个偏置神经元，程序应该附加1的值，如下所示：\n",
        "\n",
        "$[1,2,1]$\n",
        "\n",
        "输入层的权重(2个实际输入+1个偏置)对于偏置总是有额外的权重。权值向量可能是：\n",
        "\n",
        "$[ 0.1, 0.2, 0.3]$\n",
        "\n",
        "因此，进行计算总和：\n",
        "\n",
        "$0.1*1 + 0.2*2 + 0.3*1 = 0.8$\n",
        "\n",
        "神经元再将一个0.8的值传递给$\\phi$ (phi)函数，$\\phi$ ()函数表示激活函数。\n",
        "\n",
        "上图显示了只有一个构建块的神经网络结构。 您可以将许多人工神经元链接在一起以构建人工神经网络（ANN）。 也即将人工神经元视为构建块，输入和输出是连接块。 \n",
        "\n",
        "图3.ANN-3显示了由三个神经元组成的人工神经网络：\n",
        "\n",
        "**Figure 3.ANN-3: Three Neuron Neural Network**\n",
        "\n",
        "![Three Neuron Neural Network](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/ann-simple.png \"Three Neuron Neural Network\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3Fcd1xmJS5L"
      },
      "source": [
        "上图显示了三个相互连接的神经元。 该图本质上是表示：减去一些输入，重复3次，然后再连接。 它总共有四个输入和一个输出。 神经元N1和N2的输出馈入N3以产生输出O。\n",
        "也即执行了前面的方程式三次。 前两次计算N1和N2，而第三次计算使用N1和N2的输出来计算N3，最终N3再经过一个激活函数得到O。\n",
        "\n",
        "神经网络通常不显示上图中看到如此详细。一般为了简化神经网络图，可以省略激活函数和中间输出，\n",
        "\n",
        "如图3 SANN-3所示：\n",
        "\n",
        "**Figure 3.SANN-3: Three Neuron Neural Network**\n",
        "\n",
        "![Three Neuron Neural Network](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/typical-ann.png \"Three Neuron Neural Network\")\n",
        "\n",
        "观察上图，可以看到神经网络的两个附加组件。 首先，考虑该图将输入和输出表示为抽象的虚线圆。 输入和输出可以是更广泛的神经网络的一部分。 但是，输入和输出通常是使用神经网络从计算机程序接收数据的特定类型的神经元，而输出神经元会将结果返回给程序。 这种类型的神经元称为输入神经元。 我们将在下一节中讨论这些神经元。 该图显示了神经元分层排列。 输入神经元是第一层，N1和N2神经元创建第二层，第三层包含N3，第四层包含O。大多数神经网络将神经元排列成几层 。\n",
        "\n",
        "**形成一层的神经元具有几个特征：** 首先，一层中的每个神经元都具有相同的激活功能。 但是，每一层采用的激活功能可能不同。 每个层都完全连接到下一层。 换句话说，一层中的每个神经元都与上一层中的神经元有连接。 图3.SANN-3图未完全连接。 几层缺少连接。 例如，I1和N2没有连接。 \n",
        "\n",
        "\n",
        "图3.F-ANN中的下一个神经网络已完全连接，并具有一个附加层。\n",
        "\n",
        "**Figure 3.F-ANN: Fully Connected Neural Network Diagram**\n",
        "\n",
        "![Fully Connected Neural Network Diagram](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/ann-dense.png \"Fully Connected Neural Network Diagram\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAxTEEW-LP3d"
      },
      "source": [
        "在此图中，您看到了一个完全连接的多层神经网络。 这样的网络将始终具有输入和输出层。 隐藏层结构确定网络体系结构的名称。 该图中的网络是两层网络。 大多数网络将具有零到两个隐藏层。 除非您实施了深度学习策略，否则很少有具有两个以上隐藏层的网络。\n",
        "\n",
        "您可能还注意到，箭头总是从输入指向输出或向下指向输出。这种类型的神经网络称为前馈神经网络。在本课程的后面，我们将会看到在神经元之间形成反向循环的循环神经网络。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUilBXP9LXoK"
      },
      "source": [
        "## 3. 神经元的类型\n",
        "在上一节中，我们简要介绍了神经元的概念。现在我们将解释课程中描述的所有神经元类型。不是每一个神经网络都会使用每一种神经元。一个神经元也可能同时扮演多个不同类型神经元的角色。\n",
        "\n",
        "在神经网络中通常有四种类型的神经元：\n",
        "\n",
        "* **Input Neurons** - 每个输入神经元被映射到特征向量中的一个元素。\n",
        "* **Hidden Neurons** - 隐藏的神经元允许将神经网络抽象化并将输入处理为输出\n",
        "* **Output Neurons** - 每个输出神经元计算输出的一部分。\n",
        "* **Context Neurons** - 在调用神经网络之间保持状态以进行预测。\n",
        "* **Bias Neurons** - 类似于线性方程的y轴截距。\n",
        "\n",
        "这些神经元被分成以下几层：\n",
        "\n",
        "* **Input Layer** - 输入层接受来自数据集的特征向量。输入层通常有一个偏置神经元。\n",
        "* **Output Layer** - 神经网络的输出。输出层没有偏置神经元。\n",
        "* **Hidden Layers** - 发生在输入层和输出层之间的层。每个隐藏层通常有一个偏置神经元。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJrqG4pINWsn"
      },
      "source": [
        "### 3.1 输入和输出神经元\n",
        "几乎每个神经网络都有输入和输出神经元。输入神经元为神经网络接受来自程序的数据。输出神经元将处理过的数据从神经网络传回给程序。程序将这些输入和输出神经元分组到单独的层，称为输入和输出层。程序通常将神经网络的输入表示为数组或向量。向量中包含的元素数量必须等于输入神经元的数量。例如，一个有三个输入神经元的神经网络可以接受以下输入向量:\n",
        "\n",
        "\n",
        "$[0.5, 0.75, 0.2]$\n",
        "\n",
        "神经网络通常接受浮点向量作为其输入。同样，神经网络输出的向量长度等于输出神经元的数量。输出通常是来自单个输出神经元的单个值。为了保持一致，我们将单个输出神经元网络的输出表示为单个元素向量。\n",
        "\n",
        "**注：**输入神经元没有添加激活函数。正如前面所演示的，输入神经元只不过是一个占位符。输入被简单地加权和求和。此外，如果神经网络的神经元既是输入的又是输出的，那么神经网络的输入和输出向量的大小将是相同的。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T344NcmqOEIa"
      },
      "source": [
        "### 3.2 隐藏层神经元\n",
        "隐藏神经元有两个基本特征。首先，隐藏神经元只接受来自其他神经元的输入，例如输入或其他隐藏神经元。第二，隐藏神经元只向其他神经元输出，如输出或其他隐藏神经元。隐藏的神经元帮助神经网络理解输入，并形成输出。但是，这些隐藏层并不直接处理传入数据或最终输出。程序员经常将隐藏的神经元分组成完全连接的隐藏层。\n",
        "\n",
        "程序员的一个常见问题是网络中隐藏的神经元的数量。由于这个问题的答案是复杂的，本课程不止一个部分将包括有关隐藏神经元数量的讨论。在深度学习之前，研究人员普遍认为，任何超过一个隐藏层的东西都是过度的(Hornik, 1991)。研究人员已经证明，单隐藏层神经网络可以作为一个通用逼近器。换句话说，这个网络应该能够学会从任何输入产生(或近似)任何输出，只要它在单层中有足够的隐藏神经元。\n",
        "\n",
        "研究人员过去嘲笑额外隐藏层的另一个原因是，这些层会阻碍神经网络的训练。**训练是指确定好的权重值的过程**。在研究人员引入深度学习技术之前，我们根本没有一个有效的方法来训练一个深度网络，深度网络是具有大量隐藏层的神经网络。虽然从理论上讲，单隐藏层神经网络可以学习任何东西，但深度学习可以促进数据模式更复杂的表达。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AgIskrQOwV5"
      },
      "source": [
        "### 3.3 偏置神经元\n",
        "程序员向神经网络添加偏置神经元，以帮助他们学习更好的模式。 偏置神经元的功能就像总是产生值1的输入神经元。因为偏置神经元的输出恒定为1，所以它们未连接到上一层。 可以将值1（称为偏置激活）设置为1以外的值。但是，1是最常见的偏置激活。 并非所有的神经网络都有偏置神经元。 \n",
        "\n",
        "图3.BIAS显示了带有偏置神经元的单层神经网络：\n",
        "\n",
        "\n",
        "**Figure 3.BIAS: Neural Network with Bias Neurons**\n",
        "![Neural Network with Bias Neurons](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_2_ann.png \"Neural Network with Bias Neurons\")\n",
        " \n",
        "该网络包含三个偏置神经元。除输出层外，每一层都包括一个单偏置神经元。偏置神经元允许程序改变激活函数的输出。稍后在讨论激活函数时，我们将确切地看到这种变化是如何发生的。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEKpNxD_PnND"
      },
      "source": [
        "### 3.4 上下文神经元\n",
        "循环神经网络利用上下文神经元来保持状态。 这种类型的神经元允许神经网络维持状态。 因为给定的输入可能并不总是产生相同的输出。 这种不一致类似于生物大脑的运作。 如：当听到大声喇叭时，上下文因素如何影响您的响应。 如果您在过马路时听到噪音，则可能会惊呆，停止行走并朝喇叭的方向看。 如果您在电影院里听到号角冒险电影，那您的回答就不会完全一样。 因此，先前的输入为您提供了处理号角音频输入的上下文。\n",
        "\n",
        "时间序列是上下文神经元的一种应用。 您可能需要训练神经网络来学习输入信号以执行语音识别或预测安全价格的趋势。 上下文神经元是神经网络处理时间序列数据的一种方式。 \n",
        "\n",
        "图3.CTX显示了神经网络如何安排上下文神经元：\n",
        "\n",
        "**Figure 3.CTX: Neural Network with Context Neurons**\n",
        "![Neural Network with Context Neurons](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/ann-context.png \"Neural Network with Context Neurons\")\n",
        "\n",
        "该神经网络只有一个输入和输出神经元。在输入层和输出层之间有两个隐藏的神经元和两个上下文神经元。除了这两个上下文神经元外，这个网络与前面演示的网络是一样的。\n",
        "\n",
        "每个上下文神经元都具有一个从0开始的值，并且总是从网络的先前使用中接收隐藏的一个或隐藏的两个的副本。 该图中的两条虚线表示上下文神经元是直接副本，没有其他权重。 其他行指示输出由上面列出的六个权重值之一加权。 输出的计算仍然以相同的方式进行。 输出神经元的值将是所有四个输入的总和，乘以它们的权重，并应用于激活函数。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfwaCqCuQigy"
      },
      "source": [
        "### 3.5 其他类型的神经元\n",
        "组成神经网络的各个单元并不总是称为神经元。 研究人员有时会将这些神经元称为结点，单位或总和。 在本课程的后续模块中，我们将探索利用Boltzmann机器填补神经元作用的深度学习。 您几乎总是将构造这些单元之间的加权连接神经网络。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZwdLDReRIPy"
      },
      "source": [
        "## 4. 为什么需要偏置神经元\n",
        "上一节中看到的激活函数指定单个神经元的输出。神经元的权重和偏置共同决定了激活的输出，从而产生所需的输出。\n",
        "\n",
        "为了了解这个过程是如何发生的，考虑下面的方程。它代表一个单输入的sigmoid激活神经网络：\n",
        "\n",
        "$ f(x,w,b) = \\frac{1}{1 + e^{-(wx+b)}} $ \n",
        "\n",
        "$x$变量表示神经网络的单个输入。$w$和$b$变量指定了神经网络的权值和偏差。上式为输入的加权和与sigmoid激活函数的组合。在这一节中，我们将考虑sigmoid函数，因为它演示了偏置神经元的影响。\n",
        "\n",
        "神经元的权重允许您调整激活函数的斜率或形状。 图3.A-WEIGHT显示了权重变化时对S型激活函数输出的影响：\n",
        "\n",
        "\n",
        "**Figure 3.A-WEIGHT: Neuron Weight Shifting**\n",
        "![Adjusting Weight](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_2_bias_weight.png \"Neuron Weight Shifting\")\n",
        "\n",
        "上图显示了使用以下参数的多个Sigmoid曲线：\n",
        "\n",
        "\n",
        "f（x，0.5,0.0）\\\n",
        "f（x，1.0,0.0）\\\n",
        "f（x，1.5,0.0）\\\n",
        "f（x，2.0,0.0）\n",
        "\n",
        "为了生成这些曲线，我们没有使用偏差，这在每个情况下的第三个参数0中很明显。使用四个权重值可以得到上图中的四个不同的s型曲线。但不管权重是多少，当x = 0时，我们总是得到相同的0.5。因为当x = 0时，所有的曲线都到达了相同的点。当输入接近0.5时，我们可能需要神经网络产生其他值。\n",
        "\n",
        "偏差确实会使simoid曲线偏移，当x接近0时，允许值不超过0.5。\n",
        "\n",
        "图3.a - bias显示了在几个不同偏差下使用权重为1.0的效果\n",
        "\n",
        "**Figure 3.A-BIAS: Neuron Bias Shifting**\n",
        "![Adjusting Bias](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_2_bias_value.png \"Neuron Bias Shifting\")\n",
        "\n",
        "上图显示了具有以下参数的多个Sigmoid曲线：\n",
        "\n",
        "f（x，1.0,1.0）\n",
        "\n",
        "f（x，1.0,0.5）\n",
        "\n",
        "f（x，1.0,1.5）\n",
        "\n",
        "f（x，1.0,2.0）\n",
        "\n",
        "在所有权重均为1.0的情况下，选择几种不同的偏差时，Sigmoid曲线向左或向右移动。 由于所有曲线在右上角或左下角合并，因此并不是完全平移。\n",
        "\n",
        "当我们将偏差和权重放在一起时，它们产生了一条曲线，该曲线创建了神经元的必要输出。 以上曲线仅是一个神经元的输出。 在一个完整的网络中，许多不同神经元的输出将合并以产生复杂的输出模式。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhIba_WmmUWP"
      },
      "source": [
        "## 5. 现代的激活函数\n",
        "激活函数，也称为传递函数，用于计算神经网络每一层的输出。 历史上，神经网络使用双曲正切，Sigmoid/logistic或线性激活函数。 但是，现代深度神经网络主要利用以下激活函数：\n",
        "\n",
        "* **Rectified Linear Unit (ReLU)** - 用于隐藏层的输出 [[Cite:glorot2011deep]](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)\n",
        "* **Softmax** - 用于分类神经网络的输出 [Softmax Example](http://www.heatonresearch.com/aifh/vol3/softmax.html)\n",
        "* **Linear** - 用于回归神经网络的输出\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3rdVJAHnE_D"
      },
      "source": [
        "### 5.1 Linear Activation Function\n",
        "最基本的激活函数是线性函数，因为它根本不会改变神经元的输出。\n",
        "\n",
        "下面的公式显示了程序通常如何实现线性激活函数:\n",
        "\n",
        "$\\phi(x) = x$\n",
        "\n",
        "如您所见，此激活函数仅返回神经元输入传递给它的值。 \n",
        "\n",
        "图3. LIN显示了线性激活函数的图形：\n",
        "\n",
        "**Figure 3.LIN: Linear Activation Function**\n",
        "![Linear Activation Function](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/graphs-linear.png \"Linear Activation Function\")\n",
        "\n",
        "回归神经网络，即那些学会提供数值的网络，通常会在它们的输出层上使用线性激活函数。那些为其输入确定适当类的分类神经网络，通常会为其输出层使用softmax激活函数。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vso1K6fonkeA"
      },
      "source": [
        "### 5.2 Rectified Linear Units (ReLU)\n",
        "修正线性单元（ReLU）由Teh＆Hinton于2000年推出，在过去几年中得到了迅速的采用。 在使用ReLU激活函数之前，程序员通常将双曲正切视为选择的激活函数。 由于出色的训练结果，目前大多数最新研究都推荐ReLU。 因此，大多数神经网络应在隐藏层上使用ReLU，在输出层上使用softmax或线性。 \n",
        "\n",
        "下式显示了简单的ReLU函数：\n",
        "\n",
        "$ \\phi(x) = \\max(0, x) $\n",
        "\n",
        "现在，我们将讲解为什么ReLU通常比隐藏层的其他激活函数要好。 \n",
        "**性能提高的部分原因在于ReLU激活函数是线性的非饱和函数**。 与Sigmoid/logistic或双曲线切线激活函数不同，ReLU不会饱和到-1、0或1。饱和的激活函数朝着并最终获得一个值。 例如，当x减小时，双曲正切函数饱和为-1，而当x增大时，双曲正切函数为1。\n",
        "\n",
        "图3.RELU显示了ReLU激活函数的图形：\n",
        "\n",
        "\n",
        "**Figure 3.RELU: Rectified Linear Units (ReLU)**\n",
        "![Rectified Linear Units (ReLU)](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/graphs-relu.png \"Rectified Linear Units (ReLU)\")\n",
        "\n",
        "最新研究表明，神经网络的隐藏层应使用ReLU激活。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBBen7Ggokl-"
      },
      "source": [
        "### 5.3 Softmax Activation Function\n",
        "我们将讨论的最后的激活函数是softmax激活函数。 除了线性激活函数之外，通常还可以在神经网络的输出层中找到softmax函数。 分类神经网络通常采用softmax函数。 具有最高值的神经元要求将输入作为其类的成员。 由于它是一种较好的方法，因此softmax激活函数会强制神经网络的输出表示输入落入每个类的概率。 如果没有softmax，则神经元的输出只是数字值，最高的表示所属的类别。\n",
        "\n",
        "为了了解神经网络如何使用softmax激活函数，我们将研究一个典型的神经网络分类问题。 虹膜数据集包含针对150种不同虹膜花的四个测量值。 这些花中的每一种都属于三种虹膜之一。 当您提供花朵的测量值时，softmax函数允许神经网络为您提供这些测量值属于这三种物种的概率。 例如，神经网络可能会告诉您，虹膜有setosa发生的可能性为80％，其为弗吉尼亚属的可能性为15％，而杂色的可能性仅为5％。 因为这些是概率，所以它们的总和必须为100％。 不可能有80％的setosa发生率，75％的弗吉尼亚属发生率和20％的杂色发生率-这种结果将是荒谬的。\n",
        "\n",
        "要将输入数据分类为三种虹膜中的一种，这三种虹膜中的每一种都需要一个输出神经元。输出神经元并不固有地指定三种物种中每一种的概率。因此，我们希望提供总和为100%的概率。神经网络会告诉你三种花中每一种的概率。为了得到概率，可以使用softmax()函数，如下：\n",
        "\n",
        "$ \\phi_i(x) = \\frac{e^{z_i}}{\\sum\\limits_{j \\in group}e^{z_j}} $\n",
        "\n",
        "上式中，$i$表示程序所计算的输出神经元($o$)的索引值，$j$表示组/级/层中所有神经元的索引值。变量z指定了输出神经元的数组。值得注意的是，该程序计算softmax激活的方式与此模块中的其他激活函数不同。当softmax为激活函数时，单个神经元的输出依赖于其他输出神经元。\n",
        "\n",
        "要查看运行中的softmax函数，请参考此[Softmax示例网站]（http://www.heatonresearch.com/aifh/vol3/softmax.html）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnkEHHx9qNmi"
      },
      "source": [
        "考虑一个训练有素的神经网络，它将数据分为三类，比如三种虹膜。在本例中，您将为每个目标类使用一个输出神经元。考虑神经网络是否输出如下内容\n",
        "\n",
        "\n",
        "* **Neuron 1**: setosa: 0.9\n",
        "* **Neuron 2**: versicolour: 0.2\n",
        "* **Neuron 3**: virginica: 0.4\n",
        "\n",
        "从上面的输出中可以看出，神经网络将数据考虑为一个setosa iris。然而，这些数字并不是概率。0.9的值并不代表setosa数据90%的可能性。这些值加起来是1。5。为了让程序将它们视为概率，它们的和必须为1.0。这个神经网络的输出向量如下：\n",
        "\n",
        "\n",
        "$ [0.9,0.2,0.4] $\n",
        "\n",
        "如果您将此向量提供给softmax函数，它将返回以下向量\n",
        "\n",
        "$ [0.47548495534876745 , 0.2361188410001125 , 0.28839620365112] $\n",
        "\n",
        "以上三个值的和为1.0，可以作为概率处理。表示setosa虹膜的数据的可能性为48%，因为向量中的第一个值四舍五入为0.48(48%)。您可以通过以下方式计算此值\n",
        "\n",
        "\n",
        "$ sum=\\exp(0.9)+\\exp(0.2)+\\exp(0.4)=5.17283056695839 \\\\\n",
        "j_0= \\exp(0.9)/sum = 0.47548495534876745 \\\\\n",
        "j_1= \\exp(0.2)/sum = 0.2361188410001125 \\\\\n",
        "j_2= \\exp(0.4)/sum = 0.28839620365112\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o3VyODgqkvz"
      },
      "source": [
        "## 6. 经典的激活函数\n",
        "### 6.1 Step Activation Function\n",
        "步骤或阈值激活函数是另一个简单的激活函数。神经网络最初被称为感知器。McCulloch & Pitts(1943)引入了最初的感知器，并使用了如下方程所示的步骤激活函数:[[Cite:mcculloch1943logical]](https://link.springer.com/article/10.1007/BF02478259)\n",
        "\n",
        "\n",
        "$ \\phi(x)=\\begin{cases}\n",
        "    1, \\text{if x >= 0.5}.\\\\\n",
        "    0, \\text{otherwise}.\n",
        "  \\end{cases}$\n",
        "\n",
        "  对于输入的0.5或更高的值，此方程输出的值为1.0；对于所有其他值，此方程的输出为0。 阶跃函数（也称为阈值函数）仅对大于指定阈值的值返回1（真），如图3所示。\n",
        "  \n",
        "**Figure 3.STEP: Step Activation Function**\n",
        "![Step Activation Function](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/graphs-step.png \"Step Activation Function\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DALVk9bErKeq"
      },
      "source": [
        "### 6.2 Sigmoid Activation Function\n",
        "对于只需要输出正数的前馈神经网络来说，sigmoid或logistic激活函数是一种非常常见的选择。双曲正切或直线单元激活函数(ReLU)虽然应用广泛，但通常只是较为合适的选择。\n",
        "\n",
        "sigmoid激活函数为:\n",
        "$ \\phi(x) = \\frac{1}{1 + e^{-x}} $\n",
        "\n",
        "使用sigmoid函数以确保值保持在相对较小的范围内，如图3所示.SIGMOID：\n",
        "\n",
        "**Figure 3.SIGMOID: Sigmoid Activation Function**\n",
        "![Sigmoid Activation Function](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/graphs-sigmoid.png \"Sigmoid Activation Function\")\n",
        "\n",
        "从上图可以看出，可以将值强制设置为一个范围。 在此，函数将大于或小于0的值压缩到大约0到1之间的范围。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOYGhBWvro1D"
      },
      "source": [
        "### 6.3 Hyperbolic Tangent Activation Function\n",
        "双曲正切函数也是神经网络的普遍激活函数，必须输出介于-1和1之间的值。此激活函数就是双曲正切（tanh）函数，如以下等式所示：\n",
        "\n",
        "\n",
        "$ \\phi(x) = \\tanh(x) $\n",
        "\n",
        "双曲正切函数的图形与sigmoid激活函数的图形类似，如图3.HTAN所示。\n",
        "\n",
        "**Figure 3.HTAN: Hyperbolic Tangent Activation Function**\n",
        "![Hyperbolic Tangent Activation Function](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/graphs-tanh.png \"Hyperbolic Tangent Activation Function\")\n",
        "\n",
        "双曲正切函数比s型激活函数有几个优点。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzmnpiiKsETj"
      },
      "source": [
        "## 7.为什么ReLu更有效？\n",
        "为什么ReLU激活功能如此受欢迎?这是使深度学习工作的神经网络的关键改进之一。[[Cite:nair2010rectified]](https://www.cs.toronto.edu/~hinton/absps/reluICML.pdf)在深度学习之前，sigmoid激活功能普遍存在。我们在本模块的前面讨论了sigmoid激活函数。比如在Keras等深度学习框架中，经常使用梯度下降来训练神经网络。对于利用梯度下降的神经网络，需要对激活函数求导。程序必须导出每个权值对误差函数的偏导数。\n",
        "\n",
        "图3.DERV表示一个导数，即瞬时变化率。\n",
        "\n",
        "**Figure 3.DERV: Derivative**\n",
        "![Derivative](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_2_deriv.png \"Derivative\")\n",
        "\n",
        "Sigmoid函数的导数在此处给出：\n",
        "\n",
        "$ \\phi'(x)=\\phi(x)(1-\\phi(x)) $\n",
        "\n",
        "教科书上经常给出其它形式的导数。为了提高计算效率，我们使用上述形式。要了解如何确定这个导数，[请参考下面的文章](http://www.heatonresearch.com/aifh/vol3/deriv_sigmoid.html)。\n",
        "\n",
        "我们在图3.SDERV中给出了sigmoid导数的曲线图。\n",
        "\n",
        "**Figure 3.SDERV: Sigmoid Derivative**\n",
        "![Sigmoid Derivative](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_2_deriv_sigmoid.png \"Sigmoid Derivative\")\n",
        "\n",
        "随着$ x $从零开始，导数迅速饱和到零。 而对于对于ReLU的导数而言，却不会如此，如下所示：\n",
        "\n",
        "$ \\phi'(x) = \\begin{cases} 1 & x > 0 \\\\ 0 & x \\leq 0 \\end{cases} $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtedDpgXBtvq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}